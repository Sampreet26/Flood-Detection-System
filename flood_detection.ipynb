{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#**Import Modules**"
      ],
      "metadata": {
        "id": "i4HvK6WguFJA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "jhy9Ij4czXKI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Load Data**"
      ],
      "metadata": {
        "id": "58AnoR4SufC7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flow_depth_df = pd.read_csv(\"Flow_depth_v3.csv\", skiprows=1)\n",
        "flow_rate_df = pd.read_csv(\"Flow_rate_v3.csv\", skiprows=1)\n",
        "node_df = pd.read_csv(\"WW01_node.csv\")"
      ],
      "metadata": {
        "id": "dEDTCqIDuU1B"
      },
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "sqp9VgRJupBl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flow_depth_df.rename(columns={'IDs:': 'Time'}, inplace=True)\n",
        "flow_rate_df.rename(columns={'IDs:': 'Time'}, inplace=True)\n",
        "node_df.columns = node_df.columns.str.strip()\n",
        "node_df.rename(columns={'Node ID': 'NodeID'}, inplace=True)\n",
        "flow_depth_df.rename(columns={'Date/Time': 'Time'}, inplace=True)\n",
        "flow_rate_df.rename(columns={'Date/Time': 'Time'}, inplace=True)\n",
        "flow_depth_df['Time'] = flow_depth_df['Time'].astype(str)\n",
        "flow_rate_df['Time'] = flow_rate_df['Time'].astype(str)\n",
        "node_df['NodeID'] = node_df['NodeID'].astype(str)\n",
        "\n",
        "for df in [flow_depth_df, flow_rate_df]:\n",
        "    for col in df.columns[1:]:  # Skip the 'Time' column\n",
        "        df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "merged_df = flow_depth_df.merge(flow_rate_df, on='Time', suffixes=('_depth', '_rate'))\n",
        "relevant_node_features = ['NodeID', 'X-Coordinate', 'Y-Coordinate', 'Invert Elev. (ft)']\n",
        "node_df = node_df[relevant_node_features]\n",
        "\n",
        "if 'NodeID' in merged_df.columns and 'NodeID' in node_df.columns:\n",
        "    merged_df = merged_df.merge(node_df, on='NodeID', how='left')\n",
        "else:\n",
        "    print(\"Error: 'NodeID' column missing in either merged_df or node_df\")\n",
        "\n",
        "merged_df['Time'] = pd.to_datetime(merged_df['Time'], errors='coerce')\n",
        "merged_df['Hour'] = merged_df['Time'].dt.hour\n",
        "merged_df['Day'] = merged_df['Time'].dt.day\n",
        "merged_df['Month'] = merged_df['Time'].dt.month\n",
        "merged_df.fillna(merged_df.median(), inplace=True)\n",
        "\n",
        "features = [col for col in merged_df.columns if col not in ['Time', 'NodeID']]\n",
        "X = merged_df[features].values\n",
        "X_min = X.min(axis=0)\n",
        "X_max = X.max(axis=0)\n",
        "X = (X - X_min) / (X_max - X_min + 1e-8)\n",
        "data_cleaned = pd.DataFrame(X, columns=features)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S_3P2Lmduzl1",
        "outputId": "44818162-5ba6-48e1-9ffb-4b6d869b32be"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: 'NodeID' column missing in either merged_df or node_df\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-73-6e61677df486>:24: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
            "  merged_df['Time'] = pd.to_datetime(merged_df['Time'], errors='coerce')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Splitting Data**"
      ],
      "metadata": {
        "id": "72H0nE59vStU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "threshold = 0.5\n",
        "\n",
        "y = (merged_df['Depth'] > threshold).astype(int).values\n",
        "\n",
        "X_tensor = torch.tensor(X, dtype=torch.float32)\n",
        "y_tensor = torch.tensor(y, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "split_ratio = 0.8\n",
        "\n",
        "split_idx = int(len(X) * split_ratio)\n",
        "\n",
        "indices = torch.randperm(len(X))\n",
        "\n",
        "# Split the data\n",
        "train_indices = indices[:split_idx]\n",
        "test_indices = indices[split_idx:]\n",
        "X_train, X_test = X_tensor[train_indices], X_tensor[test_indices]\n",
        "y_train, y_test = y_tensor[train_indices], y_tensor[test_indices]\n",
        "\n",
        "# Shapes to confirm\n",
        "print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
        "print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TJ0BZIvXvaEl",
        "outputId": "490b61bc-f0db-4d78-f8f9-1bc2f93a51d3"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_train shape: torch.Size([14165, 49]), y_train shape: torch.Size([14165, 1])\n",
            "X_test shape: torch.Size([3542, 49]), y_test shape: torch.Size([3542, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Neural Network (Baseline)**"
      ],
      "metadata": {
        "id": "DJkrYGv3v0iE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleNN(nn.Module):\n",
        "    def __init__(self, input_dim, hidden_dim=64):\n",
        "        super(SimpleNN, self).__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, 2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# Initialize the model\n",
        "input_dim = X_train.shape[1]\n",
        "nn_model = SimpleNN(input_dim)\n",
        "\n",
        "# Define loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(nn_model.parameters(), lr=0.01)\n",
        "\n",
        "# Training loop\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    nn_model.train()\n",
        "    optimizer.zero_grad()\n",
        "    outputs = nn_model(X_train)\n",
        "    loss = criterion(outputs, y_train.long().view(-1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    if (epoch + 1) % 10 == 0:\n",
        "        print(f\"Epoch [{epoch + 1}/{epochs}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    y_true = y_true.cpu().numpy()\n",
        "    y_pred = y_pred.cpu().numpy()\n",
        "\n",
        "    tp = np.sum((y_true == 1) & (y_pred == 1))\n",
        "    fp = np.sum((y_true == 0) & (y_pred == 1))\n",
        "    fn = np.sum((y_true == 1) & (y_pred == 0))\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "\n",
        "    return f1\n",
        "\n",
        "# Evaluation\n",
        "nn_model.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = nn_model(X_test)\n",
        "    _, predicted = torch.max(outputs.data, 1)\n",
        "    accuracy = (predicted == y_test.long().view(-1)).sum().item() / y_test.size(0)\n",
        "    f1 = calculate_f1_score(y_test.long().view(-1), predicted)\n",
        "    print(f\"Neural Network Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"Neural Network F1 Score: {f1:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mqg4GubLzqE0",
        "outputId": "1c74f7c4-7722-4fd8-a8d4-1a0bc7dd5e28"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/200], Loss: 0.3370\n",
            "Epoch [20/200], Loss: 0.1858\n",
            "Epoch [30/200], Loss: 0.1097\n",
            "Epoch [40/200], Loss: 0.0751\n",
            "Epoch [50/200], Loss: 0.0537\n",
            "Epoch [60/200], Loss: 0.0403\n",
            "Epoch [70/200], Loss: 0.0317\n",
            "Epoch [80/200], Loss: 0.0260\n",
            "Epoch [90/200], Loss: 0.0222\n",
            "Epoch [100/200], Loss: 0.0196\n",
            "Epoch [110/200], Loss: 0.0176\n",
            "Epoch [120/200], Loss: 0.0160\n",
            "Epoch [130/200], Loss: 0.0148\n",
            "Epoch [140/200], Loss: 0.0139\n",
            "Epoch [150/200], Loss: 0.0131\n",
            "Epoch [160/200], Loss: 0.0124\n",
            "Epoch [170/200], Loss: 0.0118\n",
            "Epoch [180/200], Loss: 0.0114\n",
            "Epoch [190/200], Loss: 0.0109\n",
            "Epoch [200/200], Loss: 0.0106\n",
            "Neural Network Accuracy: 0.9972\n",
            "Neural Network F1 Score: 0.9845\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Train Neural Network Model**"
      ],
      "metadata": {
        "id": "gMr7okBCv9QG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "# Define the parameter grid\n",
        "hidden_dims = [32, 64, 128]\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs_list = [50, 100, 200]\n",
        "\n",
        "# Function to train the model\n",
        "def train_nn_model(hidden_dim, lr, epochs):\n",
        "    model = SimpleNN(input_dim, hidden_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train.long().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Grid search\n",
        "best_f1 = 0\n",
        "best_params = None\n",
        "for hidden_dim, lr, epochs in itertools.product(hidden_dims, learning_rates, epochs_list):\n",
        "    model = train_nn_model(hidden_dim, lr, epochs)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        f1 = calculate_f1_score(y_test.long().view(-1), predicted)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_params = {'hidden_dim': hidden_dim, 'lr': lr, 'epochs': epochs}\n",
        "\n",
        "print(f\"Best F1 Score for NN: {best_f1:.4f}\")\n",
        "print(f\"Best Parameters for NN: {best_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "itODzxwCz79x",
        "outputId": "fc3d30b6-4e38-4447-b49e-ea8e8c2d48a4"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best F1 Score for NN: 0.9892\n",
            "Best Parameters for NN: {'hidden_dim': 64, 'lr': 0.1, 'epochs': 50}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Random Forest**"
      ],
      "metadata": {
        "id": "2Zt5HbEpwKP5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RandomForest(nn.Module):\n",
        "    def __init__(self, input_dim, n_estimators=100):\n",
        "        super(RandomForest, self).__init__()\n",
        "        self.trees = nn.ModuleList([nn.Linear(input_dim, 2) for _ in range(n_estimators)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = [tree(x) for tree in self.trees]\n",
        "        outputs = torch.stack(outputs, dim=0)\n",
        "        outputs = torch.mean(outputs, dim=0)\n",
        "        return outputs\n",
        "\n",
        "# Define the parameter grid\n",
        "n_estimators_list = [50, 100, 200]\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs_list = [50, 100, 200]\n",
        "\n",
        "# Function to train the model\n",
        "def train_rf_model(n_estimators, lr, epochs):\n",
        "    model = RandomForest(input_dim, n_estimators)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train.long().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Grid search\n",
        "best_f1 = 0\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "for n_estimators, lr, epochs in itertools.product(n_estimators_list, learning_rates, epochs_list):\n",
        "    model = train_rf_model(n_estimators, lr, epochs)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == y_test.long().view(-1)).sum().item() / y_test.size(0)\n",
        "        f1 = calculate_f1_score(y_test.long().view(-1), predicted)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'n_estimators': n_estimators, 'lr': lr, 'epochs': epochs}\n",
        "\n",
        "print(f\"Best F1 Score for RF: {best_f1:.4f}\")\n",
        "print(f\"Best Accuracy for RF: {best_accuracy:.4f}\")\n",
        "print(f\"Best Parameters for RF: {best_params}\")\n"
      ],
      "metadata": {
        "id": "fagHB4xA20u2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Gradient Boosting Tree**"
      ],
      "metadata": {
        "id": "Yg9Zd6-8wOPE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientBoostingTree(nn.Module):\n",
        "    def __init__(self, input_dim, n_estimators=100, learning_rate=0.1):\n",
        "        super(GradientBoostingTree, self).__init__()\n",
        "        self.trees = nn.ModuleList([nn.Linear(input_dim, 2) for _ in range(n_estimators)])\n",
        "        self.learning_rate = learning_rate\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = torch.zeros(x.size(0), 2)  # Initialize outputs with the correct shape\n",
        "        for tree in self.trees:\n",
        "            outputs += self.learning_rate * tree(x)\n",
        "        return outputs\n",
        "\n",
        "# Define the parameter grid\n",
        "n_estimators_list = [50, 100, 200]\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs_list = [50, 100, 200]\n",
        "\n",
        "# Function to train the model\n",
        "def train_gbt_model(n_estimators, lr, epochs):\n",
        "    model = GradientBoostingTree(input_dim, n_estimators, lr)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train.long().view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Grid search\n",
        "best_f1 = 0\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "for n_estimators, lr, epochs in itertools.product(n_estimators_list, learning_rates, epochs_list):\n",
        "    model = train_gbt_model(n_estimators, lr, epochs)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        accuracy = (predicted == y_test.long().view(-1)).sum().item() / y_test.size(0)\n",
        "        f1 = calculate_f1_score(y_test.long().view(-1), predicted)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'n_estimators': n_estimators, 'lr': lr, 'epochs': epochs}\n",
        "\n",
        "print(f\"Best F1 Score for GBT: {best_f1:.4f}\")\n",
        "print(f\"Best Accuracy for GBT: {best_accuracy:.4f}\")\n",
        "print(f\"Best Parameters for GBT: {best_params}\")"
      ],
      "metadata": {
        "id": "-gRbSV652_Al"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Linear Regression**"
      ],
      "metadata": {
        "id": "sGSRz1sjwZKo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define a simple linear regression model using PyTorch\n",
        "class LinearRegressionModel(nn.Module):\n",
        "    def __init__(self, input_dim):\n",
        "        super(LinearRegressionModel, self).__init__()\n",
        "        self.linear = nn.Linear(input_dim, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)\n",
        "\n",
        "# Function to calculate F1 score using PyTorch\n",
        "def calculate_f1_score(y_true, y_pred):\n",
        "    tp = (y_true * y_pred).sum().to(torch.float32)\n",
        "    tn = ((1 - y_true) * (1 - y_pred)).sum().to(torch.float32)\n",
        "    fp = ((1 - y_true) * y_pred).sum().to(torch.float32)\n",
        "    fn = (y_true * (1 - y_pred)).sum().to(torch.float32)\n",
        "\n",
        "    precision = tp / (tp + fp + 1e-8)\n",
        "    recall = tp / (tp + fn + 1e-8)\n",
        "    f1 = 2 * (precision * recall) / (precision + recall + 1e-8)\n",
        "    return f1.item()\n",
        "\n",
        "# Assuming X_train, y_train, X_test, y_test, and criterion are already defined\n",
        "input_dim = X_train.shape[1]\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs_list = [50, 100, 200]\n",
        "criterion = nn.MSELoss()  # Use MSELoss for regression\n",
        "# Function to train the model\n",
        "def train_lr_model(lr, epochs):\n",
        "    model = LinearRegressionModel(input_dim)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train.view(-1, 1).float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Grid search\n",
        "best_f1 = 0\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "for lr, epochs in itertools.product(learning_rates, epochs_list):\n",
        "    model = train_lr_model(lr, epochs)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        predicted = outputs.round().view(-1)\n",
        "        accuracy = (predicted == y_test.view(-1)).sum().item() / y_test.size(0)\n",
        "        f1 = calculate_f1_score(y_test.view(-1), predicted)\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'lr': lr, 'epochs': epochs}\n",
        "\n",
        "print(f\"Best F1 Score for LR: {best_f1:.4f}\")\n",
        "print(f\"Best Accuracy for LR: {best_accuracy:.4f}\")\n",
        "print(f\"Best Parameters for LR: {best_params}\")\n"
      ],
      "metadata": {
        "id": "8RIGKBRC3VzZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Support Vector Machine**"
      ],
      "metadata": {
        "id": "pWhBEcH0wtgL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define the SVM-style linear model\n",
        "class SVM(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes):\n",
        "        super(SVM, self).__init__()\n",
        "        self.fc = nn.Linear(input_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.fc(x)\n",
        "\n",
        "# Custom F1-score using PyTorch only\n",
        "def calculate_f1_score(y_true, y_pred, num_classes):\n",
        "    f1 = 0.0\n",
        "    for cls in range(num_classes):\n",
        "        tp = ((y_pred == cls) & (y_true == cls)).sum().item()\n",
        "        fp = ((y_pred == cls) & (y_true != cls)).sum().item()\n",
        "        fn = ((y_pred != cls) & (y_true == cls)).sum().item()\n",
        "\n",
        "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
        "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
        "        f1_class = (2 * precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
        "        f1 += f1_class\n",
        "\n",
        "    return f1 / num_classes  # macro average\n",
        "\n",
        "# Custom accuracy using PyTorch only\n",
        "def calculate_accuracy(y_true, y_pred):\n",
        "    correct = (y_true == y_pred).sum().item()\n",
        "    total = y_true.size(0)\n",
        "    return correct / total\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rates = [0.001, 0.01, 0.1]\n",
        "epochs = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Setup input/output sizes\n",
        "input_dim = X_train.shape[1]\n",
        "num_classes = len(torch.unique(y_train))\n",
        "\n",
        "# Flatten target tensors to 1D\n",
        "y_train = y_train.view(-1).long()\n",
        "y_test = y_test.view(-1).long()\n",
        "\n",
        "# Training function\n",
        "def train_svm_model(lr):\n",
        "    model = SVM(input_dim, num_classes)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(X_train)\n",
        "        loss = criterion(outputs, y_train)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    return model\n",
        "\n",
        "# Grid search\n",
        "best_f1 = 0\n",
        "best_accuracy = 0\n",
        "best_params = None\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model = train_svm_model(lr)\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(X_test)\n",
        "        _, predicted = torch.max(outputs, 1)\n",
        "        f1 = calculate_f1_score(y_test, predicted, num_classes)\n",
        "        accuracy = calculate_accuracy(y_test, predicted)\n",
        "\n",
        "        if f1 > best_f1:\n",
        "            best_f1 = f1\n",
        "            best_accuracy = accuracy\n",
        "            best_params = {'lr': lr}\n",
        "\n",
        "# Results\n",
        "print(f\"Best F1 Score for SVM: {best_f1:.4f}\")\n",
        "print(f\"Best Accuracy for SVM: {best_accuracy:.4f}\")\n",
        "print(f\"Best Parameters for SVM: {best_params}\")\n"
      ],
      "metadata": {
        "id": "0T4PndV53pSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Ensemble evaluating**"
      ],
      "metadata": {
        "id": "Tr-56NFVw0Ac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.metrics import f1_score\n",
        "\n",
        "class WeightedSoftVotingEnsembleModel(nn.Module):\n",
        "    def __init__(self, models, X_val, y_val):\n",
        "        super(WeightedSoftVotingEnsembleModel, self).__init__()\n",
        "        self.models = models\n",
        "\n",
        "        # Compute dynamic weights based on F1 scores\n",
        "        self.weights = self.compute_dynamic_weights(X_val, y_val)\n",
        "\n",
        "    def compute_dynamic_weights(self, X_val, y_val):\n",
        "        \"\"\"Computes model weights dynamically based on F1 scores.\"\"\"\n",
        "        f1_scores = []\n",
        "        with torch.no_grad():\n",
        "            for model in self.models:\n",
        "                outputs = model(X_val)\n",
        "                if outputs.shape[1] == 1:\n",
        "                    probs = torch.sigmoid(outputs)\n",
        "                    preds = (probs > 0.5).long().squeeze()\n",
        "                else:\n",
        "                    preds = torch.argmax(outputs, dim=1)\n",
        "\n",
        "                f1 = f1_score(y_val.cpu(), preds.cpu(), average='weighted')\n",
        "                f1_scores.append(f1)\n",
        "\n",
        "        # Convert F1 scores into weights\n",
        "        f1_scores = torch.tensor(f1_scores)\n",
        "        normalized_weights = f1_scores / f1_scores.sum()  # Normalize so they sum to 1\n",
        "\n",
        "        return normalized_weights.view(-1, 1, 1)  # Reshape for broadcasting\n",
        "\n",
        "    def forward(self, x):\n",
        "        outputs = []\n",
        "        for model in self.models:\n",
        "            output = model(x)\n",
        "            if output.shape[1] == 1:\n",
        "                probs = torch.cat([1 - torch.sigmoid(output), torch.sigmoid(output)], dim=1)\n",
        "            else:\n",
        "                probs = torch.softmax(output, dim=1)\n",
        "            outputs.append(probs)\n",
        "\n",
        "        outputs = torch.stack(outputs, dim=0)  # shape: [num_models, batch, num_classes]\n",
        "        weighted_avg = torch.sum(outputs * self.weights.to(outputs.device), dim=0) / self.weights.sum()\n",
        "        return weighted_avg\n",
        "\n",
        "# Compute weights dynamically and instantiate the ensemble\n",
        "weighted_ensemble = WeightedSoftVotingEnsembleModel(\n",
        "    [nn_model, rf_model, gbt_model, lr_model, svm_model], X_test, y_test\n",
        ")\n",
        "\n",
        "# Evaluate the ensemble\n",
        "weighted_ensemble.eval()\n",
        "with torch.no_grad():\n",
        "    outputs = weighted_ensemble(X_test)\n",
        "    predictions = torch.argmax(outputs, dim=1)\n",
        "\n",
        "    f1 = f1_score(y_test.cpu(), predictions.cpu(), average='weighted')\n",
        "    accuracy = (predictions == y_test.long().view(-1)).sum().item() / y_test.size(0)\n",
        "\n",
        "print(f\"Dynamic Weighted Voting Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Dynamic Weighted Voting F1 Score: {f1:.4f}\")\n"
      ],
      "metadata": {
        "id": "wMBtjqtQquKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "blEvBMhZuWgj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}